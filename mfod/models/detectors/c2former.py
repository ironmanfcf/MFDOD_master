
"""
authors: changfeng feng

Implementation of `C²Former: calibrated and complementary transformer for RGB-infrared object detection.`__

__ https://ieeexplore.ieee.org/document/10472947/
"""

from mfod.registry import MODELS
import torch

from mmdet.structures import OptSampleList, SampleList
from mmdet.utils import ConfigType, OptConfigType, OptMultiConfig

from typing import List, Sequence, Tuple, Union, Dict
from mmdet.structures import DetDataSample, OptSampleList, SampleList
from torch import Tensor
from mmdet.models.utils import unpack_gt_instances
from .refine_single_stage_ts import RefineSingleStageDetectorTwoStream



ForwardResults = Union[Dict[str, torch.Tensor], List[DetDataSample],
                       Tuple[torch.Tensor], torch.Tensor]

@MODELS.register_module()
class C2Former(RefineSingleStageDetectorTwoStream):


    def __init__(self,
                 backbone: ConfigType,
                 neck: OptConfigType = None,
                 bbox_head_init: OptConfigType = None,
                 bbox_head_refine: List[OptConfigType] = None,
                 train_cfg: OptConfigType = None,
                 test_cfg: OptConfigType = None,
                 data_preprocessor: OptConfigType = None,
                 init_cfg: OptMultiConfig = None) -> None:
        super().__init__(
            backbone=backbone,
            neck=neck,
            bbox_head_init=bbox_head_init,
            bbox_head_refine=bbox_head_refine,
            train_cfg=train_cfg,
            test_cfg=test_cfg,
            data_preprocessor=data_preprocessor,
            init_cfg=init_cfg
        )

    def extract_feat(self, batch_inputs: Tensor,
                     batch_inputs_ir: Tensor) -> Tuple[Tensor]:
        """Extract features.

        Args:
            batch_inputs (Tensor): Image tensor with shape (N, C, H ,W).
            batch_inputs_ir (Tensor): Input images of shape (N, C, H, W).

        Returns:
            tuple[Tensor]: Multi-level features that may have
            different resolutions.
        """
        x = self.backbone(batch_inputs, batch_inputs_ir)
        if self.with_neck:
            x = self.neck(x)
        return x
        
        
        
        
"""
# origin code by maoxuan yuan
# """
# import pdb

# from mmrotate. import rbbox2result
# from ..builder import ROTATED_DETECTORS, build_backbone, build_head, build_neck
# from .base import RotatedBaseDetector
# from .utils import AlignConvModule
# from mmcv.runner import auto_fp16
# import torch
# import warnings

# from mmrotate.models import RefineSingleStageDetector


# @ROTATED_DETECTORS.register_module()
# class C2Former(RotatedBaseDetector):
#     """Implementation of `C²Former: calibrated and complementary transformer for RGB-infrared object detection`__

#     __ https://ieeexplore.ieee.org/document/9377550
#     """

#     def __init__(self,
#                  backbone,
#                  neck=None,
#                  fam_head=None,
#                  align_cfgs=None,
#                  odm_head=None,
#                  train_cfg=None,
#                  test_cfg=None,
#                  pretrained=None,
#                  init_cfg=None):
#         super(C2Former, self).__init__(init_cfg)

#         if pretrained:
#             warnings.warn('DeprecationWarning: pretrained is deprecated, '
#                           'please use "init_cfg" instead')
#             backbone.pretrained = pretrained

#         self.backbone = build_backbone(backbone)
#         if neck is not None:
#             self.neck = build_neck(neck)
#         if train_cfg is not None:
#             fam_head.update(train_cfg=train_cfg['fam_cfg'])
#         fam_head.update(test_cfg=test_cfg)
#         self.fam_head = build_head(fam_head)

#         self.align_conv_type = align_cfgs['type']
#         self.align_conv_size = align_cfgs['kernel_size']
#         self.feat_channels = align_cfgs['channels']
#         self.featmap_strides = align_cfgs['featmap_strides']

#         if self.align_conv_type == 'AlignConv':
#             self.align_conv = AlignConvModule(self.feat_channels,
#                                               self.featmap_strides,
#                                               self.align_conv_size)

#         if train_cfg is not None:
#             odm_head.update(train_cfg=train_cfg['odm_cfg'])
#         odm_head.update(test_cfg=test_cfg)
#         self.odm_head = build_head(odm_head)

#         self.train_cfg = train_cfg
#         self.test_cfg = test_cfg

#     def extract_feat(self, img, img_tir, img_meta):
#         """Directly extract features from the backbone+neck."""
#         x = self.backbone(img, img_tir)

#         if self.with_neck:
#             x = self.neck(x)

#         return x

#     def forward_dummy(self, img, img_tir):
#         """Used for computing network flops.

#         See `mmedetection/tools/get_flops.py`
#         """
#         x = self.extract_feat(img, img_tir)
#         outs = self.fam_head(x)
#         rois = self.fam_head.refine_bboxes(*outs)
#         # rois: list(indexed by images) of list(indexed by levels)
#         align_feat = self.align_conv(x, rois)
#         outs = self.odm_head(align_feat)

#         return outs

#     def forward_train(self,
#                       img,
#                       img_tir,
#                       img_metas,
#                       gt_bboxes,
#                       gt_labels,
#                       gt_bboxes_ignore=None):
#         """Forward function of S2ANet."""
#         losses = dict()
#         x = self.extract_feat(img, img_tir, img_metas)

#         outs = self.fam_head(x)

#         loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
#         loss_base = self.fam_head.loss(
#             *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
#         for name, value in loss_base.items():
#             losses[f'fam.{name}'] = value

#         rois = self.fam_head.refine_bboxes(*outs)
#         # rois: list(indexed by images) of list(indexed by levels)
#         align_feat = self.align_conv(x, rois)
#         outs = self.odm_head(align_feat)
#         loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
#         loss_refine = self.odm_head.loss(
#             *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore, rois=rois)
#         for name, value in loss_refine.items():
#             losses[f'odm.{name}'] = value

#         return losses

#     def simple_test(self, img, img_tir, img_meta, rescale=False):
#         """Test function without test time augmentation.

#         Args:
#             imgs (list[torch.Tensor]): List of multiple images
#             img_metas (list[dict]): List of image information.
#             rescale (bool, optional): Whether to rescale the results.
#                 Defaults to False.

#         Returns:
#             list[list[np.ndarray]]: BBox results of each image and classes. \
#                 The outer list corresponds to each image. The inner list \
#                 corresponds to each class.
#         """
#         x = self.extract_feat(img, img_tir, img_meta)
#         outs = self.fam_head(x)
#         rois = self.fam_head.refine_bboxes(*outs)
#         # rois: list(indexed by images) of list(indexed by levels)
#         align_feat = self.align_conv(x, rois)
#         outs = self.odm_head(align_feat)

#         bbox_inputs = outs + (img_meta, self.test_cfg, rescale)
#         bbox_list = self.odm_head.get_bboxes(*bbox_inputs, rois=rois)
#         bbox_results = [
#             rbbox2result(det_bboxes, det_labels, self.odm_head.num_classes)
#             for det_bboxes, det_labels in bbox_list
#         ]
#         return bbox_results

#     def aug_test(self, imgs, img_metas, **kwargs):
#         """Test function with test time augmentation."""
#         raise NotImplementedError

#     def forward_test(self, imgs, imgs_tir,  img_metas, **kwargs):
#         """
#         Args:
#             imgs (List[Tensor]): the outer list indicates test-time
#                 augmentations and inner Tensor should have a shape NxCxHxW,
#                 which contains all images in the batch.
#             img_metas (List[List[dict]]): the outer list indicates test-time
#                 augs (multiscale, flip, etc.) and the inner list indicates
#                 images in a batch.
#         """
#         for var, name in [(imgs, 'imgs'), (imgs_tir, 'imgs_tir'), (img_metas, 'img_metas')]:
#             if not isinstance(var, list):
#                 raise TypeError(f'{name} must be a list, but got {type(var)}')

#         num_augs = len(imgs)
#         if num_augs != len(img_metas):
#             raise ValueError(f'num of augmentations ({len(imgs)}) '
#                              f'!= num of image meta ({len(img_metas)})')

#         # NOTE the batched image size information may be useful, e.g.
#         # in DETR, this is needed for the construction of masks, which is
#         # then used for the transformer_head.
#         for img, img_meta in zip(imgs, img_metas):
#             batch_size = len(img_meta)
#             for img_id in range(batch_size):
#                 img_meta[img_id]['batch_input_shape'] = tuple(img.size()[-2:])

#         if num_augs == 1:
#             # proposals (List[List[Tensor]]): the outer list indicates
#             # test-time augs (multiscale, flip, etc.) and the inner list
#             # indicates images in a batch.
#             # The Tensor should have a shape Px4, where P is the number of
#             # proposals.
#             if 'proposals' in kwargs:
#                 kwargs['proposals'] = kwargs['proposals'][0]
#             return self.simple_test(imgs[0], imgs_tir[0], img_metas[0], **kwargs)
#         else:
#             assert imgs[0].size(0) == 1, 'aug test does not support ' \
#                                          'inference with batch size ' \
#                                          f'{imgs[0].size(0)}'
#             # TODO: support test augmentation for predefined proposals
#             assert 'proposals' not in kwargs
#             return self.aug_test(imgs, imgs_tir, img_metas, **kwargs)

#     @auto_fp16(apply_to=('img', 'img_tir', ))
#     def forward(self, img, img_tir, img_metas, return_loss=True, **kwargs):
#         """Calls either :func:`forward_train` or :func:`forward_test` depending
#         on whether ``return_loss`` is ``True``.

#         Note this setting will change the expected inputs. When
#         ``return_loss=True``, img and img_meta are single-nested (i.e. Tensor
#         and List[dict]), and when ``resturn_loss=False``, img and img_meta
#         should be double nested (i.e.  List[Tensor], List[List[dict]]), with
#         the outer list indicating test time augmentations.
#         """
#         if torch.onnx.is_in_onnx_export():
#             assert len(img_metas) == 1
#             return self.onnx_export(img[0], img_metas[0])

#         if return_loss:
#             return self.forward_train(img, img_tir, img_metas, **kwargs)
#         else:
#             return self.forward_test(img, img_tir, img_metas, **kwargs)

